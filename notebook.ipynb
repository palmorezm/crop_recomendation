{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning is well equipped when it comes to analyzing data regarding soil conditions, including moisture level, temperature, and chemical makeup, all of which have an impact upon crop growth and livestock well-being. Today in agriculture, this can allow crops to be grown at much higher precision, enabling farmers to treat plants and animals almost individually, which in turn significantly increases the effectiveness of farmers' decisions. Using this can develop means to even predict harvest yields and evaluate crop quality for individual plant species to detect crop disease and weed infestations which were previouly impossible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../input/crop-recommendation-dataset/Crop_recommendation.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(),cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at the distribution of temperature and ph.\n",
    "\n",
    "It is symmetrical and bell shaped, showing that trials will usually give a result near the average, but will occasionally deviate by large amounts. It's also fascinating how these two really resemble each other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "# sns.distplot(df_setosa['sepal_length'],kde=True,color='green',bins=20,hist_kws={'alpha':0.3})\n",
    "sns.distplot(df['temperature'],color=\"purple\",bins=15,hist_kws={'alpha':0.2})\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(df['ph'],color=\"green\",bins=15,hist_kws={'alpha':0.2})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check if the dataset is balanced or not. If found imbalanced, we would have to downsample some targets which are more in quantity but so far everything looks good! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y='label',data=df, palette=\"plasma_r\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very important plot to visualize the diagonal distribution between two features for all the combinations! It is great to visualize how classes differ from each other in a particular space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue = 'label')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During rainy season, average rainfall is high (average 120 mm) and temperature is mildly chill (less than 30'C).¶\n",
    "Rain affects soil moisture which affects ph of the soil. Here are the crops which are likely to be planted during this season.\n",
    "- Rice needs heavy rainfall (>200 mm) and a humidity above 80%. No wonder major rice production in India comes from East Coasts which has average of 220 mm rainfall every year!\n",
    "- Coconut is a tropical crop and needs high humidity therefore explaining massive exports from coastal areas around the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"rainfall\",y=\"humidity\",data=df[(df['temperature']<30) & (df['rainfall']>120)],hue=\"label\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph correlates with average potassium (K) and average nitrogen (N) value (both>50).¶\n",
    "These soil ingredients direcly affects nutrition value of the food. Fruits which have high nutrients typically has consistent potassium values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"K\",y=\"N\",data=df[(df['N']>40)&(df['K']>40)],hue=\"label\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to plot a specfic case of pairplot between `humidity` and `K` (potassium levels in the soil.)¶\n",
    "sns.jointplot() can be used for bivariate analysis to plot between humidity and K levels based on Label type. It further generates frequency distribution of classes with respect to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"K\",y=\"humidity\",data=df,hue='label',size=8,s=30,alpha=0.7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see ph values are critical when it comes to soil. A stability between 6 and 7 is preffered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y='label',x='ph',data=df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting analysis where Phosphorous levels are quite differentiable when it rains heavily (above 150 mm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y='label',x='P',data=df[df['rainfall']>150])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further analyzing phosphorous levels.¶\n",
    "When humidity is less than 65, almost same phosphor levels(approx 14 to 25) are required for 6 crops which could be grown just based on the amount of rain expected over the next few weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = df[(df['humidity']<65)], x = \"K\", y = \"rainfall\",hue=\"label\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PRE-PROCESSING\n",
    "\n",
    "### Let's make the data ready for machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=df.label.astype('category')\n",
    "targets = dict(enumerate(c.cat.categories))\n",
    "df['target']=c.cat.codes\n",
    "\n",
    "y=df.target\n",
    "X=df[['N','P','K','temperature','humidity','ph','rainfall']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation visualization between features. We can see how Phosphorous levels and Potassium levels are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X.corr())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SCALING\n",
    "**Feature scaling is required before creating training data and feeding it to the model.**\n",
    "\n",
    "As we saw earlier, two of our features (temperature and ph) are gaussian distributed, therefore scaling them between 0 and 1 with MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# we must apply the scaling to the test set as well that we are computing for the training set\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL SELECTION\n",
    "\n",
    "## KNN Classifier for Crop prediction. \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "knn.score(X_test_scaled, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat=confusion_matrix(y_test,knn.predict(X_test_scaled))\n",
    "df_cm = pd.DataFrame(mat, list(targets.values()), list(targets.values()))\n",
    "sns.set(font_scale=1.0) # for label size\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 12},cmap=\"terrain\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try different values of n_neighbors to fine tune and get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1,11)\n",
    "scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    scores.append(knn.score(X_test_scaled, y_test))\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, scores)\n",
    "plt.vlines(k_range,0, scores, linestyle=\"dashed\")\n",
    "plt.ylim(0.96,0.99)\n",
    "plt.xticks([i for i in range(1,11)]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Support Vector Classifer (SVC)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_linear = SVC(kernel = 'linear').fit(X_train_scaled, y_train)\n",
    "print(\"Linear Kernel Accuracy: \",svc_linear.score(X_test_scaled,y_test))\n",
    "\n",
    "svc_poly = SVC(kernel = 'rbf').fit(X_train_scaled, y_train)\n",
    "print(\"Rbf Kernel Accuracy: \", svc_poly.score(X_test_scaled,y_test))\n",
    "\n",
    "svc_poly = SVC(kernel = 'poly').fit(X_train_scaled, y_train)\n",
    "print(\"Poly Kernel Accuracy: \", svc_poly.score(X_test_scaled,y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to increase SVC Linear model accuracy by parameter tuning.\n",
    "\n",
    "**GridSearchCV can help us find the best parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'C': np.logspace(-3, 2, 6).tolist(), 'gamma': np.logspace(-3, 2, 6).tolist()}\n",
    "# 'degree': np.arange(0,5,1).tolist(), 'kernel':['linear','rbf','poly']\n",
    "\n",
    "model = GridSearchCV(estimator = SVC(kernel=\"linear\"), param_grid=parameters, n_jobs=-1, cv=4)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.best_score_ )\n",
    "print(model.best_params_ )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POINTS TO BE HIGHLIGHTED**\n",
    "1. *Interestingly liner kernel also gives satisfactory results but fine tuning increases the computation and might be inefficient in some cases*\n",
    "2. *The accuracy can be increased in poly kernel by tweaking parameters but might lead to intensive overfitting.*\n",
    "3. *RBF has better result than linear kernel.*\n",
    "4. *Poly kernel so far wins by a small margin.* "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying using decision tree\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42).fit(X_train, y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the import features which are taken into consideration by decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4), dpi=80)\n",
    "c_features = len(X_train.columns)\n",
    "plt.barh(range(c_features), clf.feature_importances_)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature name\")\n",
    "plt.yticks(np.arange(c_features), X_train.columns)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Random Forest.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max depth and n_estimator are important to fine tune otherwise trees will be densely graphed which will be a classic case of overfitting. max_depth=4 and n_estimators=10 gives pretty much satisfying results by making sure model is able to generalize well.\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=4,n_estimators=100,random_state=42).fit(X_train, y_train)\n",
    "\n",
    "print('RF Accuracy on training set: {:.2f}'.format(clf.score(X_train, y_train)))\n",
    "print('RF Accuracy on test set: {:.2f}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"color:blue;\"><b> Classification report </b></div>\n",
    "\n",
    "#### **Let's use <u>yellowbrick</u> for classification report as they are great for visualizing in a tabular format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassificationReport\n",
    "classes=list(targets.values())\n",
    "visualizer = ClassificationReport(clf, classes=classes, support=True,cmap=\"Blues\")\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Gradient Boosting\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "grad = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "print('Gradient Boosting accuracy : {}'.format(grad.score(X_test,y_test))) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
